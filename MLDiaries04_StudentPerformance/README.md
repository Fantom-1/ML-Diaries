# MLDiaries 04 â€” Exploring the Limits of Regression

This repository documents **MLDiaries 04**, a learning-first machine learning project where the goal was **not to chase high accuracy**, but to understand *how far regression models can realistically go* when predicting student performance â€” and where they naturally fall short.

This project focuses on **thinking like an ML practitioner**:
- diagnosing bias vs variance  
- interpreting models instead of worshipping scores  
- knowing when to **stop and reject a model**

---

## ðŸŽ¯ Project Objectives

- Build a **leakage-free regression model** for student performance  
- Analyze **feature correlations and relationships**  
- Study **biasâ€“variance behavior** using learning curves  
- Test whether **increasing complexity actually helps**  
- Build a **What-If Analyzer** for counterfactual reasoning  
- Practice **responsible model rejection**

---

## ðŸ“Š Dataset Overview

The dataset contains **academic, demographic, family, and lifestyle features**, including:

- Study time, failures, absences  
- Parental education and jobs  
- School and family support  
- Motivation indicators (e.g. intent for higher education)

### ðŸŽ¯ Target Variable
- `G3` â€” Final grade

âš ï¸ **Important:**  
Early grades (`G1`, `G2`) were intentionally excluded to **avoid data leakage**.

---

## ðŸ—ï¸ Modeling Pipeline

All experiments are built using a **scikit-learn Pipeline** to ensure:
- clean preprocessing
- reproducibility
- no leakage

### Pipeline Design
- **Numeric features**
  - `StandardScaler`
  - `PolynomialFeatures` (tested, later rejected)
- **Categorical features**
  - `OneHotEncoder`
- **Model**
  - Linear Regression (baseline)
  - Ridge Regression (stability analysis)

ðŸ“Œ **Pipeline architecture:**

![Pipeline Diagram](data.image/pipeline.png)

---

## ðŸ“ˆ Baseline Model Performance

The linear regression baseline produced:

- **MAE â‰ˆ 3.4**
- **RMSE â‰ˆ 4.2**
- **RÂ² â‰ˆ 0.14**

Learning curves showed **early convergence**, indicating:

- low variance  
- **high bias (underfitting)**  

ðŸ“Œ This was not a tuning issue â€” it was a **model capacity limitation**.

---

## ðŸ”¬ Polynomial Regression Experiment

To address underfitting, polynomial features were introduced.

### Result:
- MAE worsened to â‰ˆ 4.1  
- RMSE increased to â‰ˆ 5.0  
- **RÂ² dropped to âˆ’0.22**

ðŸ“Œ Interpretation:
> Increasing complexity amplified noise instead of capturing signal.

The polynomial model was **deliberately rejected** â€” a key ML decision.

---

## ðŸ”Ž Feature Influence & Stability

Feature coefficients were analyzed and compared with **Ridge Regression** to test signal stability.

Key observations:
- Motivation for higher education â†’ strong positive signal  
- Prior failures â†’ strong negative signal  
- Support features appear negative due to **reactive patterns**  

ðŸ“Œ **Coefficient stability comparison:**

![Feature Coefficients](data/coefficients.png)

This step confirmed that the modelâ€™s insights were **stable, not accidental**.

---

## ðŸ” What-If Analyzer (Counterfactual Reasoning)

Instead of only predicting, the model was used as a **simulation tool**.

> â€œWhat happens if one factor changes while everything else stays constant?â€

### Example Scenarios

| Scenario | Predicted Grade |
|--------|-----------------|
| Baseline | 1.88 |
| +2 Study Time | 3.40 |
| âˆ’1 Failure | 3.74 |
| âˆ’5 Absences | 1.75 |

ðŸ“Œ **What-If results:**

![What-If Analyzer](data/scenarios.png)

### Insights:
- Preventing failures matters more than small effort increases  
- Motivation has stronger leverage than attendance alone  
- Not all features move the needle equally  

âš ï¸ These are **model-based insights**, not causal guarantees.

---

## ðŸ§  Key Learnings

- More complexity â‰  better learning  
- Learning curves can tell you **when to stop**  
- Rejecting a model is sometimes the *most correct outcome*  
- ML is about **judgment**, not just metrics  

> Sometimes the most honest model is the one that admits its limits.

---

